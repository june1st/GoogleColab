{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT FineTuning with Cloud TPU: Sentence and Sentence-Pair Classification Tasks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/june1st/GoogleColab/blob/master/TPU_benchmark_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rkTLZ3I4_7c_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TPU benchmark\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\" >\n",
        " <td>\n",
        "<img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "  <td>\n",
        "<img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "metadata": {
        "id": "co8HlrpYc4yl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5c81bdda-8839-4fd3-bb4b-0a58eb63b9b7"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/koshian2/TPU-Benchmark.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TPU-Benchmark'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 106 (delta 0), reused 0 (delta 0), pack-reused 103\u001b[K\n",
            "Receiving objects: 100% (106/106), 335.80 KiB | 3.69 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NoLtUnlGydTG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d03ffcf6-a45f-49e0-fb85-421634115b66"
      },
      "cell_type": "code",
      "source": [
        "%cd TPU-Benchmark/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TPU-Benchmark\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZYNBH6XnynHu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8401757b-14cc-4da3-baaf-93596b6340d5"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-100-python\t images      README.md\t\t   utils\n",
            "cifar100-python\t\t LICENSE     result\n",
            "cifar-100-python.tar.gz  raw_result  tpu_gpu_benchmark.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JXqbogxc0IOL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "dd5df38d-e1a8-4ab2-e69e-76475ad5a153"
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -zxvf cifar-100-python.tar.gz"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-13 23:06:25--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  5.66MB/s    in 30s     \n",
            "\n",
            "2018-12-13 23:06:55 (5.46 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "cifar-100-python/\n",
            "cifar-100-python/file.txt~\n",
            "cifar-100-python/train\n",
            "cifar-100-python/test\n",
            "cifar-100-python/meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WHJM-dC4zeZn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir ./cifar100-raw\n",
        "!mkdir ./cifar100-raw/train\n",
        "!mkdir ./cifar100-raw/test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H52nVinTy2FL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle, os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "with open(\"cifar-100-python/train\", \"rb\") as fp:\n",
        "    train = pickle.load(fp, encoding=\"latin-1\")\n",
        "with open(\"cifar-100-python/test\", \"rb\") as fp:\n",
        "    test = pickle.load(fp, encoding=\"latin-1\")\n",
        "\n",
        "def parse_pickle(rawdata, rootdir):\n",
        "    for i in range(100):\n",
        "        dir = rootdir + \"/\" + f\"{i:02d}\"\n",
        "        if not os.path.exists(dir):\n",
        "            os.mkdir(dir) \n",
        "            #print(dir)\n",
        "    m = len(rawdata[\"filenames\"])\n",
        "    for i in range(m):\n",
        "        if i % 100 == 0:\n",
        "            print(i)\n",
        "        filename = rawdata[\"filenames\"][i]\n",
        "        label = rawdata[\"fine_labels\"][i]\n",
        "        data = rawdata[\"data\"][i]\n",
        "        data = data.reshape(3, 32, 32)\n",
        "        data = np.swapaxes(data, 0, 2)\n",
        "        data = np.swapaxes(data, 0, 1)\n",
        "        with Image.fromarray(data) as img:\n",
        "            img.save(f\"{rootdir}/{label:02d}/{filename}\")\n",
        "\n",
        "parse_pickle(train, \"cifar100-raw/train\")\n",
        "parse_pickle(test, \"cifar100-raw/test\")\n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JR7vHH4FyoQF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "917cf8da-70d5-4bc1-90d8-2d88317d249f"
      },
      "cell_type": "code",
      "source": [
        "!python tpu_gpu_benchmark.py "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "Start training...\n",
            "use_tpu:False, batch_size:256, use_validation:True, use_augment:False, from_storage:True, workers:1\n",
            "Found 50000 images belonging to 100 classes.\n",
            "Found 10000 images belonging to 100 classes.\n",
            "Epoch 1/1\n",
            " 18/195 [=>............................] - ETA: 26:39:36 - loss: 6.1575 - acc: 0.0254^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RDITJ7Bu2hLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "08e2ab75-9394-4368-976f-bdd7067aff25"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib.tpu.python.tpu import keras_support\n",
        "from tensorflow.keras.applications import NASNetLarge\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, AveragePooling2D, GlobalAveragePooling2D, Dense\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.train.RMSPropOptimizer import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.datasets import cifar100\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import Callback, History\n",
        "import tensorflow.keras.backend as K\n",
        "import numpy as np\n",
        "import os, time, pickle\n",
        "\n",
        "class Timer(Callback):\n",
        "    def __init__(self):\n",
        "        self.inital_time_starts = time.time()\n",
        "        \n",
        "    def on_train_begin(self, logs):\n",
        "        self.inital_time = time.time() - self.inital_time_starts\n",
        "        self.epoch_starts = time.time()\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        self.times.append(time.time()-self.epoch_starts)\n",
        "        self.epoch_starts = time.time()\n",
        "\n",
        "def create_residual_blocks(input_tensor, base_ch, k, N):\n",
        "    start_tensor = input_tensor\n",
        "    for i in range(N):\n",
        "        x = Conv2D(base_ch*k, 7, padding=\"same\")(start_tensor)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = Conv2D(base_ch*k, 7, padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = Add()([start_tensor, x])\n",
        "        start_tensor = x\n",
        "    return x\n",
        "\n",
        "# WideResNet\n",
        "def create_wideresnet(k, N, use_tpu):\n",
        "    input = Input(shape=(32, 32, 3))\n",
        "    # conv1 : 32x32\n",
        "    x = Conv2D(16*k, 1)(input)\n",
        "    x = create_residual_blocks(x, 16, k, N)\n",
        "    # downsampling 32->16\n",
        "    x = AveragePooling2D(2)(x)\n",
        "    x = Conv2D(32*k, 1)(x)\n",
        "    # conv2 : 16x16\n",
        "    x = create_residual_blocks(x, 32, k, N)\n",
        "    # downsampling 16->8\n",
        "    x = AveragePooling2D(2)(x)\n",
        "    x = Conv2D(64*k, 1)(x)\n",
        "    # conv4 : 8x8\n",
        "    x = create_residual_blocks(x, 64, k, N)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(100, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(input, x)\n",
        "    model.compile(Adam(), loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "    if use_tpu:\n",
        "        tpu_grpc_url = \"grpc://\"+os.environ[\"COLAB_TPU_ADDR\"]\n",
        "        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_grpc_url)\n",
        "        strategy = keras_support.TPUDistributionStrategy(tpu_cluster_resolver)\n",
        "        model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n",
        "    return model\n",
        "\n",
        "def single_trial(use_tpu, batch_size, use_validation, use_augment, from_storage, parallel_workers):\n",
        "    K.clear_session()\n",
        "    model = create_wideresnet(7, 4, use_tpu)\n",
        "\n",
        "    train_gen = ImageDataGenerator(\n",
        "        rescale=1.0/255,\n",
        "        width_shift_range=4.0/32,\n",
        "        height_shift_range=4.0/32,\n",
        "        horizontal_flip=True)\n",
        "    val_gen = ImageDataGenerator(\n",
        "        rescale=1.0/255)\n",
        "\n",
        "    if not from_storage:\n",
        "        (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "        y_train = to_categorical(y_train)\n",
        "        y_test = to_categorical(y_test)\n",
        "        if not use_augment:\n",
        "            X_train = (X_train / 255.0).astype(np.float32)\n",
        "            X_test = (X_test / 255.0).astype(np.float32)\n",
        "\n",
        "    timer = Timer()\n",
        "    hist = History()\n",
        "\n",
        "    n_train_examples, n_test_examples = 50000, 10000\n",
        "    n_epochs = 1\n",
        "    multiprocess = False if parallel_workers <= 1 else True\n",
        "\n",
        "    print(\"Start training...\")\n",
        "    print(f\"use_tpu:{use_tpu}, batch_size:{batch_size}, use_validation:{use_validation}, use_augment:{use_augment}, from_storage:{from_storage}, workers:{parallel_workers}\")\n",
        "\n",
        "    if from_storage:\n",
        "        if use_augment:\n",
        "            if use_validation:\n",
        "                model.fit_generator(train_gen.flow_from_directory(\"cifar100-raw/train\", target_size=(32, 32), \n",
        "                                                                  class_mode=\"categorical\", shuffle=True,\n",
        "                                                                  batch_size=batch_size), \n",
        "                                    steps_per_epoch=n_train_examples//batch_size, epochs=n_epochs,\n",
        "                                    callbacks=[timer, hist],\n",
        "                                    workers=parallel_workers, use_multiprocessing=multiprocess,\n",
        "                                    validation_data=val_gen.flow_from_directory(\"cifar100-raw/test\", target_size=(32, 32),\n",
        "                                                                                class_mode=\"categorical\", shuffle=True,\n",
        "                                                                                batch_size=batch_size),\n",
        "                                    validation_steps=n_test_examples//batch_size)\n",
        "            else:\n",
        "                model.fit_generator(train_gen.flow_from_directory(\"cifar100-raw/train\", target_size=(32, 32), \n",
        "                                                                  class_mode=\"categorical\", shuffle=True,\n",
        "                                                                  batch_size=batch_size), \n",
        "                                    steps_per_epoch=n_train_examples//batch_size, epochs=n_epochs,\n",
        "                                    callbacks=[timer, hist],\n",
        "                                    workers=parallel_workers, use_multiprocessing=multiprocess)\n",
        "        else:\n",
        "            if use_validation:\n",
        "                model.fit_generator(val_gen.flow_from_directory(\"cifar100-raw/train\", target_size=(32, 32),\n",
        "                                                                class_mode=\"categorical\", shuffle=True,\n",
        "                                                                batch_size=batch_size),\n",
        "                                    steps_per_epoch=n_train_examples//batch_size, epochs=n_epochs,\n",
        "                                    callbacks=[timer, hist],\n",
        "                                    workers=parallel_workers, use_multiprocessing=multiprocess,\n",
        "                                    validation_data=val_gen.flow_from_directory(\"cifar100-raw/test\", target_size=(32, 32),\n",
        "                                                                                class_mode=\"categorical\", shuffle=True,\n",
        "                                                                                batch_size=batch_size),\n",
        "                                    validation_steps=n_test_examples//batch_size)\n",
        "            else:\n",
        "                model.fit_generator(val_gen.flow_from_directory(\"cifar100-raw/train\", target_size=(32, 32),\n",
        "                                                                class_mode=\"categorical\", shuffle=True,\n",
        "                                                                batch_size=batch_size),\n",
        "                                    steps_per_epoch=n_train_examples//batch_size, epochs=n_epochs,\n",
        "                                    callbacks=[timer, hist],\n",
        "                                    workers=parallel_workers, use_multiprocessing=multiprocess)\n",
        "    else:\n",
        "        if use_augment:\n",
        "            if use_validation:\n",
        "                model.fit_generator(train_gen.flow(X_train, y_train, batch_size=batch_size, shuffle=True),\n",
        "                                    steps_per_epoch=n_train_examples//batch_size,\n",
        "                                    epochs=n_epochs, callbacks=[timer, hist],\n",
        "                                    workers=parallel_workers, use_multiprocessing=multiprocess,\n",
        "                                    validation_data=val_gen.flow(X_test, y_test), validation_steps=n_test_examples//batch_size)\n",
        "            else:\n",
        "                model.fit_generator(train_gen.flow(X_train, y_train, batch_size=batch_size, shuffle=True),\n",
        "                                    steps_per_epoch=n_train_examples//batch_size,\n",
        "                                    epochs=n_epochs, callbacks=[timer, hist],\n",
        "                                    workers=parallel_workers, use_multiprocessing=multiprocess)\n",
        "        else:\n",
        "            # fitは並列化できない\n",
        "            if use_validation:\n",
        "                model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, callbacks=[timer, hist],\n",
        "                          validation_data=(X_test, y_test))\n",
        "            else:\n",
        "                model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, callbacks=[timer, hist])\n",
        "\n",
        "    history = hist.history\n",
        "    history[\"initial_time\"] = timer.inital_time\n",
        "    history[\"times\"] = timer.times\n",
        "\n",
        "    result = {\n",
        "        \"device\": \"tpu\" if use_tpu else \"gpu\",\n",
        "        \"batch_size\" : batch_size,\n",
        "        \"use_validation\" : use_validation,\n",
        "        \"use_augmentation\" : use_augment,\n",
        "        \"from_storage\": from_storage,\n",
        "        \"result\" : history,\n",
        "        \"num_workers\" : parallel_workers\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "def trial(use_tpu, batch_size, separate_mode=-1):\n",
        "    flag = \"tpu\" if use_tpu else \"gpu\"\n",
        "    if separate_mode == -1:\n",
        "        filename = f\"{flag}_batchsize_{batch_size}.dat\"\n",
        "    else:\n",
        "        filename = f\"{flag}_batchsize_{batch_size}_sep{separate_mode}.dat\"\n",
        "    result = []\n",
        "\n",
        "    if separate_mode in [-1, 0]:\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=False, use_augment=False, from_storage=False, parallel_workers=1))\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=True, use_augment=False, from_storage=False, parallel_workers=1))\n",
        "    if separate_mode in [-1, 1]:\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=True, use_augment=True, from_storage=False, parallel_workers=1))\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=False, use_augment=False, from_storage=True, parallel_workers=1))\n",
        "    if separate_mode in [-1, 2]:\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=True, use_augment=False, from_storage=True, parallel_workers=1))\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=True, use_augment=True, from_storage=True, parallel_workers=1))\n",
        "    if separate_mode in [-1, 3]:\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=False, use_augment=False, from_storage=True, parallel_workers=4))\n",
        "        result.append(single_trial(use_tpu, batch_size, use_validation=True, use_augment=True, from_storage=True, parallel_workers=4))\n",
        "\n",
        "    with open(filename, \"wb\") as fp:\n",
        "        pickle.dump(result, fp)\n",
        "    return filename\n",
        "\n",
        "def appendix_trial(batch_size, use_tpu=True, sep=-1):\n",
        "    tpu_flag = \"tpu\" if use_tpu else \"gpu\"\n",
        "    filename = f\"appendix_{tpu_flag}_batch_size_{batch_size}\"\n",
        "    if sep >= 0: filename += f\"_sep_{sep}\"\n",
        "    filename += \".dat\"\n",
        "\n",
        "    result = {}\n",
        "\n",
        "    for mode in range(3):\n",
        "        if sep >= 0:\n",
        "            if sep != mode: continue\n",
        "        K.clear_session()\n",
        "        model = create_wideresnet(7, 4, use_tpu)\n",
        "\n",
        "        # mode 1 = そのままfit\n",
        "        # mode 2 = バッチサイズの倍数に切り詰めてfit\n",
        "        # mode 3 = fit_generator\n",
        "        data_gen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "        nb_epochs = 20\n",
        "        (X_train, y_train), (_, _) = cifar100.load_data()\n",
        "\n",
        "        timer = Timer()\n",
        "        hist = History()\n",
        "\n",
        "        print(\"Start training...\")\n",
        "        print(\"mode = \", mode)\n",
        "\n",
        "        if mode == 0:\n",
        "            X_train = X_train / 255.0\n",
        "            y_train = to_categorical(y_train)\n",
        "            model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs, callbacks=[timer, hist])\n",
        "        elif mode == 1:\n",
        "            n_train = (X_train.shape[0] // batch_size) * batch_size\n",
        "            X_train = X_train[:n_train, :, :, :] / 255.0\n",
        "            y_train = to_categorical(y_train[:n_train, :])\n",
        "            model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs, callbacks=[timer, hist])\n",
        "        elif mode == 2:\n",
        "            y_train = to_categorical(y_train)\n",
        "            steps_per_epoch = X_train.shape[0] // batch_size\n",
        "            model.fit_generator(data_gen.flow(X_train, y_train, batch_size=batch_size, shuffle=True),\n",
        "                                steps_per_epoch=steps_per_epoch, epochs=nb_epochs, callbacks=[timer, hist])\n",
        "\n",
        "        history = hist.history\n",
        "        history[\"initial_time\"] = timer.inital_time\n",
        "        history[\"times\"] = timer.times\n",
        "        result[mode] = history\n",
        "\n",
        "    with open(filename, \"wb\") as fp:\n",
        "        pickle.dump(result, fp)\n",
        "    return filename\n",
        "\n",
        "  \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filename = trial(True, 64, 2)#(True, 256, 2) # True if use TPU\n",
        "    #filename = appendix_trial(4096, sep=0)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-7e0c277d0903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from tensorflow.keras.optimizers import Adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSPropOptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcifar100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.train.RMSPropOptimizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jDLH_xq7cj5G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}